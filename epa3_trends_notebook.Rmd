---
title: "Joining Legacy STORET with WQX and NWIS"
output:
  html_notebook: default
  html_document: default
---

# Introduction
Data were acquired from Legacy STORET and the Water Quality Portal (WQP) to
assess long-term water quality trends in EPA region 3.  This document provides
an example how to standardize and join the data acquired from these two sources.


# Legacy STORET
STORET stands for “STORage and RETrieval.”
STORET was a United States Environmental Protection Agency (USEPA)
database that was developed in 1965 and was archived in 1998.  


### Legacy STORET Workflow

1. legacy_inv_ross.R, legacy_res_ross.R, and legacy_sta_ross.R
    + Created by Ross Mandel to import and organize Legacy STORET data.
2. legacy_clean_up.R
3. legacy_state_param.R
4. legacy_sub_station_param.R
5. legacy_subset_stations.R
6. legacy_lat_longs.R

The package `dplyr` will be used for data manipulation. For more information
on `dplyr` visit https://cran.r-project.org/web/packages/dplyr/index.html.
```{r}
library(dplyr)
```

### 1. Download and Aggregate
Legacy STORET data was downloaded by State/County and organized in folders
accordingly. Multiple text files were downloaded for each State/County. The
files in the suffix "_res", "_sta", or "inv". Data files with the suffix "_res"
contains information pretaining to the parameter measures.
Data files with the suffix "_sta" contained station information. Data files
with the suffix "_inv" contained summary statistics for each parameter in 
each county. Furthermore, queries from Legacy STORET are divided into multiple
files if the text file exceeds ~26.5 Mb.

The following function, `get_legacy`, was created to import all of files the
files in a specified working directory with a common suffix and append them
to create a single table. The `home.dir` arguement allows the the user to
specify the root folder or working directory from which the files will be 
imported. The `file.pattern` arguement allows the user to specify the suffix of
interest (i.e., "_res", "_sta", or "_inv"). When the `verbose = TRUE` 
(default), sub-directory and the file being imported are printed in real 
time within the R-Studio Console. No print statements will appear in the 
Console if `verbose = FALSE`. During data preperation, I recommend
`verbose = TRUE` because if an error occurs, the print statements make it
easier to find the file causing the issue. Additionally, the print statements
provide some reassurance that function is working and did not cause your 
computer to freeze.

The `get_legacy` function first identifies all of the sub-directories 
(i.e., sub-folders) contained within the specified `home.dir`. Many of the 
sub-directories represent county-level data. Within each sub-directory, the 
files containing the specified suffix, `file.pattern`, are obtained. Each of
these files is then imported using the __data.table__ function `fread`,
to quickly import the tab-seperated data. Any file size with 60 or fewer bytes
was skipped because these files do not include data. Additionally, the first
row is excluded because it only contains dashes (i.e., "-"). For each 
sub-directory, the data frames are appended together using the __dplyr__ 
function `bind_rows`. Finally, `bind_rows` is used to append all of the newly
appended sub-directory data frames into a single data frame.

```{r}
get_legacy <- function(home.dir, file.pattern, verbose = TRUE){
  # List of directories.
  dirs <- list.dirs(path = home.dir) 
  #----------------------------------------------------------------------------
  county.list <- lapply(seq_along(dirs), function(dir.i) {
    if (verbose == TRUE) print(dirs[dir.i])
    # List all of the files with the specified pattern.
    files.list <- list.files(path = dirs[dir.i],
                             pattern = glob2rx(file.pattern))
    #--------------------------------------------------------------------------
    town.list <- lapply(seq_along(files.list), function(town.i) {
      if (verbose == TRUE)  print(paste0("...", files.list[town.i]))
      # Specify file directory.
      file.dir <- file.path(dirs[dir.i], files.list[town.i])
      #------------------------------------------------------------------------
      if (file.size(file.dir) > 60) {
      #------------------------------------------------------------------------
      # Specify column names.
      if (grepl("res", file.pattern)) {
      # Import each tab-separated file that contains the specified pattern.
      # Ignore the header and the first two rows.
      town.df <- data.table::fread(file.dir, sep = "\t", 
                                   colClasses = list(character = c(1:6, 10:16,
                                                                   18, 20,
                                                                   22:24),
                                                     numeric = c(7:9, 17,
                                                                 19, 21)))
      #------------------------------------------------------------------------
        colnames(town.df) <- c("Agency", "Station", "Station.Name",
                               "Agency.Name", "State.Name", "County.Name", 
                               "Latitude", "Longitude", "Result.Value", "R", 
                               "HUC", "PARAMETER_NUMBER", "Start.Date", "Start.Time", 
                               "End.Date", "End.Time", "Sample.Depth", "UMK", 
                               "Replicate.Number", "CS", 
                               "COMPOSITE_GRAB_NUMBER", "CM", 
                               "Primary.Activity.Category", 
                               "Secondary.Activity.Category")
      #------------------------------------------------------------------------
      # Make sure the "Result.Value" column is class numeric.
      # In some instances, characters are reported in the "Result.Value"
      # column. These appear to be some sort of code but the code was not
      # readily interpretable. Therefore, when the values are converted to
      # class numeric, a warning will appear saying "NAs introduced by 
      # coercion". These values are removed because they contain no useful
      # information.
      town.df <- town.df %>% 
        dplyr::mutate(Result.Value = as.numeric(Result.Value)) %>% 
        dplyr::filter(!is.na(Result.Value))
      } else {
        if (grepl("sta", file.pattern)) {
      # Import each tab-separated file that contains the specified pattern.
      # Ignore the header and the first two rows.
      town.df <- data.table::fread(file.dir, sep = "\t", 
                                   colClasses = list(character = c(1:6, 9:10,
                                                                   12:15,
                                                                   17:20),
                                                     numeric = c(7:8, 11, 16)))
      #------------------------------------------------------------------------
          colnames(town.df) <- c("Agency", "Station", "Station.Name", 
                                 "Agency.Name", "State.Name", "County.Name", 
                                 "Latitude", "Longitude", "HUC", 
                                 "Rchmile.Segment", "Miles.Up.Reach",
                                 "Rchonoff", "Rchname", "Station.Alias",
                                 "Station.Type", "Station.Depth", 
                                 "Depth.Units", "S", "G", "Description")
        } else {
          if (grepl("inv", file.pattern)) {
            # Import each tab-separated file that contains the specified
            # pattern. Ignore the header and the first two rows.
            town.df <- data.table::fread(file.dir, sep = "\t", 
                                   colClasses = list(character = c(1:3, 6:9),
                                                     numeric = c(4:5, 10)))
            
            #------------------------------------------------------------------
            colnames(town.df) <- c("Code", "Short.Name", "Long.Name",
                                   "Num.Stns", "Num.Obs", "First.Date",
                                   "Last.Date", "Min.Value", "Max.Value", 
                                   "Average")
          }
        }
      }

      #------------------------------------------------------------------------
      # Exclude the first row, which only contains "-".
      town.df <- town.df[-1, ]
      # Return the new data frame.
      return(town.df)
      
      }
    })
    #--------------------------------------------------------------------------
    # Bind all of the imported files together.
    county.df <- dplyr::bind_rows(town.list)
    # Return the new data frame.
    return(county.df)
  })
  #----------------------------------------------------------------------------
  # Bind all of the county data frames together.
    final.df <- dplyr::bind_rows(county.list)
    # Return the new data frame.
    return(final.df)
}
```

Only the files with the suffix "_res" were necessary to import
and prepare for analysis. The `get_legacy` function is used below to import and
prepare tab-seperated files with the suffix "_sta" or "_res". This will take several 
minutes. If your computer is running out of memory, try dividing the data up
by state. To do this, specify a sub-directory (sub-folder) of the main 
directory as the `home.dir`. For example, on my computer I could set the 
`home.dir` to 
"D:/ZSmith/Projects/WQ_Trends/R_Data/Legacy STORET/Raw Data/Maryland", which
would only import files within the Maryland folder. Importing all of the data 
at once requires a lot of memory, but importing smaller data sets may help 
with memory allocation. The first six rows were printed as an example of the output from `get_legacy`.
```{r, warning = FALSE}
# Specify the main directory.
main.dir <- "D:/ZSmith/Projects/WQ_Trends/R_Data/Legacy STORET/Raw Data"
#------------------------------------------------------------------------------
# Prepare files with the suffix "_sta".
sta.df <- get_legacy(home.dir = main.dir,
                 file.pattern = "*_sta*.txt",
                 verbose = FALSE) %>% 
  # Keep only the rows where the "Station.Type column contains the word 
  # "STREAM" or "SPRING"."
  dplyr::filter(grepl("STREAM|SPRING", Station.Type))
#------------------------------------------------------------------------------
# Prepare files with the suffix "_res".
res.df <- get_legacy(home.dir = main.dir,
                 file.pattern = "*_res*.txt",
                 verbose = FALSE) %>% 
  # Semi_join keeps only the rows in x that have a match in y.
  dplyr::semi_join(sta.df, by = c("Agency", "Station"))
head(res.df)
```

Several of the LEGACY STORET columns contain codes (i.e., R, Param, UMK, CS, CM, Primary.Activity.Category, and Secondary.Activity.Category). Metadata for these codes are available at ftp://ftp.epa.gov/storet/exports/docs/. 


1.	Parameter_Codes_ZIP.exe
    a.	Links with column “PARAM” in the `res.df`.
    b.	Saved as legacy_parameters.csv
2.	Result_Remark_Codes.txt
    a.	Links with column “R” in the `res.df`.
    b.	Saved as Result_Remark_Codes.csv
3.	STORET_Glossary_of_Terms.txt
    a.	A glossary of Legacy STORET terms.
    b.	Saved as STORET_Glossary_of_Terms.csv
4.	Composite_Method_Codes.txt
    a.	Links with column “CM” in the `res.df`.
    b.	Saved as Composite_Method_Codes.csv
5.	Composite_Statistic_Codes.txt
    a.	Links with column “CS” in the `res.df`.
    b.	Saved as Composite_Statistic_Codes.csv
6.	Primary_Activity_Category_Codes.txt
    a.	Links with column “Primary_Acitivty” in the `res.df`.
    b.	Saved as Primary_Activity_Category_Codes.csv
7.	Secondary_Activity_Category_Codes.txt
    a.	Links with column “Secondary_Activity” in the `res.df`.
    b.	Saved asSecondary_Activity_Category_Codes.csv
8.	HUC_Hydrologic_Unit_Codes.txt
    a.	Links with column “HUC” in the station data.
    b.	Saved as HUC_Hydrologic_Unit_Codes.csv

A glossary of Legacy STORET terms was also available.
```{r}
# Change the main.dir to the directory containg the Codes.
main.dir <- "D:/ZSmith/Projects/WQ_Trends/R_Data/Legacy STORET/Parameter Code and Reporting Units"
#-----------------------------------------------------------------------------
# Use this code to allow kableExtra formatting.
options(knitr.table.format = "html") 
#-----------------------------------------------------------------------------
file.path(main.dir, "STORET_Glossary_of_Terms.csv") %>% 
  data.table::fread() %>% 
  knitr::kable()  %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"),
                                                  full_width = FALSE,
                                                  position = "center")
```


The parameter codes and the unit codes are imported below.
```{r}
# Import the parameter codes and remove unccessary columns.
param <- file.path(main.dir, "legacy_parameters.csv") %>% 
  data.table::fread(colClasses = list(character = c("PARAMETER_NUMBER"))) %>% 
  dplyr::select(-TOTAL_OBSERVATIONS, -PERCENTS_1972_1977,
                -PERCENTS_1967_1971, -PERCENTS_PRE_1967)
#------------------------------------------------------------------------------
# Import the reporting unit codes.
r.units <- file.path(main.dir, "legacy_reporting_units.csv") %>% 
  data.table::fread()
  
```

The data frames containing the codes are joined with the `res.df` using the __dplyr__ function `left_join`. A subset of columns are kept using the __dplyr__ function `select`. Finally, the __dplyr__ function `filter` was used to keep samples without a reported sample media (""), Effluent or Influent (E), Other (O), Sludge (S), or Water (W).
```{r}
legacy.df <- res.df %>% 
  dplyr::left_join(param, by = "PARAMETER_NUMBER") %>% 
  dplyr::left_join(r.units, by = "REPORTING_CODE") %>% 
  dplyr::select(Station, Agency, State.Name, County.Name,
                HUC, Latitude, Longitude, Start.Date, Start.Time,
                End.Date, End.Time, PARAMETER_NUMBER, Result.Value,
                REPORTING_UNITS, REPORTING_CODE, SHORT_NAME,
                FULL_NAME, Sample.Depth, Replicate.Number,
                DECIMAL_POINT_LOCATION, GROUP_CODE,
                SAMPLE_MEDIA, ANALYSIS_TYPE, PRIORITY_POLLUTANT,
                SUB_GROUP_CODE, ORIGIN, ENTRY_DATE, REVISION_DATE,
                TRACKING_CODE, CHEMICAL_ABSTRACT_NUMBER) %>% 
  dplyr::filter(SAMPLE_MEDIA %in% c("", "E", "O", "S", "W"))

# Remove res.df to save space.
rm(res.df)

```

## Data Manipulation Specific to EPA Region 3 Long-Term Trends
The following script is not as general as the script above and 
pretains to data manipulation specifically for the EPQ Region 3
long-term trends project
```{r}
# Load stations and parameters specified by Buchanan.
claire.dir <- "//Pike/data/Projects/EPA3Trends/Data/Data_Jan2017/Legacy/Claire_Specifications"
stations.df <- file.path(claire.dir, "LegacyStationsAll_050316Claire.txt") %>% 
  data.table::fread()
param.df <- file.path(claire.dir, "LegacyParam_byAgency_122915.txt") %>% 
  data.table::fread()
```


